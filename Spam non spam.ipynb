{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005f298d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>',\n",
       " 'Anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " '?',\n",
       " '>',\n",
       " 'Well',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " \"you're\",\n",
       " 'expecting.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " '10',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " '$100.',\n",
       " 'You',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'http://www.rackspace.com/',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'Amazon',\n",
       " 'EC2',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big..',\n",
       " 'To',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to:',\n",
       " 'groupname-unsubscribe@egroups.com',\n",
       " '']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "cwd= os.getcwd()\n",
    "path = os.path.join(cwd,'data')\n",
    "def get_sample(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "    \n",
    "fn=  os.path.join(path , 'emailSample1.txt')\n",
    "content = get_sample(fn)\n",
    "\n",
    "def word_tokenize(content):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    return: list of tokens (str) e.g. ['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a']\n",
    "    '''\n",
    "    # YOUR_CODE.  Split the content to tokens. You may need re.split()\n",
    "    # START_CODE \n",
    "    tokens = re.split(r'\\s+|(?<!\\d),.', content)\n",
    "    # END_CODE \n",
    "    \n",
    "    return tokens\n",
    "tokens  = word_tokenize('''> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n''')\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378e7cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>',\n",
       " 'anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " '?',\n",
       " '>',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " \"you're\",\n",
       " 'expecting.',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " '10',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " '$100.',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'http://www.rackspace.com/',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'amazon',\n",
       " 'ec2',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big..',\n",
       " 'to',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to:',\n",
       " 'groupname-unsubscribe@egroups.com',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lower_case(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens in lower case (str)\n",
    "    '''\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokens = lower_case(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f3731d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " '',\n",
       " '',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " 'youre',\n",
       " 'expecting',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'dollarnumber',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'amazon',\n",
       " 'ecnumber',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarray of str\n",
    "    return: ndarray of tokens replaced with corresponding unified words\n",
    "    '''\n",
    "    for i in range(len(tokens)):\n",
    "        # Remove HTML tags\n",
    "        tokens[i] = re.sub(r'<[^<>]+>', ' ', tokens[i])\n",
    "        # Mark all numbers as \"number\"\n",
    "        tokens[i] = re.sub(r'[0-9]+', 'number', tokens[i])\n",
    "        # Mark all URLs as \"httpaddr\"\n",
    "        tokens[i] = re.sub(r'(http|https)://[^\\s]*', 'httpaddr', tokens[i])\n",
    "        # Mark all email addresses as \"emailaddr\"\n",
    "        tokens[i] = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', tokens[i])\n",
    "        # Replace $ as \"dollar\"\n",
    "        tokens[i] = re.sub(r'\\$', 'dollar', tokens[i])\n",
    "        # Get rid of any punctuation\n",
    "        tokens[i] = re.sub(r'[^\\w\\s]', '', tokens[i])\n",
    "        # Remove any non-alphanumeric characters\n",
    "        tokens[i] = re.sub(r'[^a-zA-Z0-9]', ' ', tokens[i])\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "tokens = normalize_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3577cf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len= 65\n",
      "Remaining len= 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " 'youre',\n",
       " 'expecting',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'dollarnumber',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'amazon',\n",
       " 'ecnumber',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_short_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of filtered tokens (str)\n",
    "    '''\n",
    "    original_tokens_len = len(tokens)\n",
    "    \n",
    "    # Keep only tokens that have length > 0\n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "   \n",
    "    print ('Original len= {}\\nRemaining len= {}'.format(original_tokens_len, len(tokens)))    \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "tokens = filter_short_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313973c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anyon',\n",
       " 'know',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'cost',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depend',\n",
       " 'on',\n",
       " 'how',\n",
       " 'mani',\n",
       " 'visitor',\n",
       " 'your',\n",
       " 'expect',\n",
       " 'thi',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywher',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'buck',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'coupl',\n",
       " 'of',\n",
       " 'dollarnumb',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhap',\n",
       " 'amazon',\n",
       " 'ecnumb',\n",
       " 'if',\n",
       " 'your',\n",
       " 'run',\n",
       " 'someth',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscrib',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'thi',\n",
       " 'mail',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of stemmed tokens e.g. array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani']...\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Stem each token in the list\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "tokens = stem_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d448a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab)= 1,899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary(fn):\n",
    "    '''\n",
    "    fn: str - full path to file \n",
    "    return: ndarray of str e.g. array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)\n",
    "    '''\n",
    "    vocab_list = pd.read_table(fn, header=None)\n",
    "    vocab = np.array(vocab_list)[:,1] # first columns is index, select only words column  \n",
    "    print ('len(vocab)= {:,}'.format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "fn=  os.path.join(path , 'vocab.txt')\n",
    "vocab = get_vocabulary(fn)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03dcbb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.0 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def represent_features(tokens, vocab):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    vocab: ndarry of str\n",
    "    return: ndarry of binary values 1 if word from vocabulary is in mail, 0 otherwise\n",
    "    '''\n",
    "    # Initialize an array of zeros with the same length as the vocabulary\n",
    "    tokens_represented = np.zeros(len(vocab))\n",
    "\n",
    "    # Iterate through the tokens and set the corresponding index to 1 if the word is in the vocabulary\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            index = np.where(vocab == token)[0]\n",
    "            tokens_represented[index] = 1\n",
    "\n",
    "    print('{} word(s) from vocab are in the tokens.'.format(np.sum(tokens_represented)))\n",
    "\n",
    "    return tokens_represented\n",
    "tokens_represented = represent_features(tokens, vocab)\n",
    "tokens_represented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f02cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len= 65\n",
      "Remaining len= 61\n",
      "44.0 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(content, vocab):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    vocab: ndarray of str - list of considered words \n",
    "    '''\n",
    "    # Tokenize content\n",
    "    tokens = word_tokenize(content)\n",
    "\n",
    "    # Make tokens lowercase\n",
    "    tokens = lower_case(tokens)\n",
    "\n",
    "    # Normalize tokens\n",
    "    tokens = normalize_tokens(tokens)\n",
    "\n",
    "    # Remove zero length tokens\n",
    "    tokens = filter_short_tokens(tokens)\n",
    "\n",
    "    # Stem words\n",
    "    tokens = stem_tokens(tokens)\n",
    "\n",
    "    # Convert to binary array of features\n",
    "    tokens_represented = represent_features(tokens, vocab)\n",
    "    \n",
    "    return tokens_represented\n",
    "preprocess (content,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37fca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= {} (4000, 1899)\n",
      "y_train.shape= {} (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "fn=  os.path.join(path , 'spamTrain.mat')\n",
    "\n",
    "mat= loadmat(fn)\n",
    "X_train= mat['X']\n",
    "y_train= mat['y'].ravel()\n",
    "\n",
    "print ('X_train.shape= {}',X_train.shape)\n",
    "print ('y_train.shape= {}',y_train.shape)\n",
    "\n",
    "fn=  os.path.join(path , 'spamTest.mat')\n",
    "mat= loadmat(fn)\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest'].ravel() \n",
    "\n",
    "print ('X_test.shape= {}',X_test.shape)\n",
    "print ('y_test.shape= {}',y_test.shape)\n",
    "index = 0 \n",
    "print ('Sample with index  ={}: \\n{}'.format(index, X_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab8b9904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train= 0.99975\n",
      "Score test= 0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maks\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "C = .1\n",
    "clf= LinearSVC(C=C)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Score train= {}'.format(clf.score(X_train,y_train)))\n",
    "print ('Score test= {}'.format(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc156a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our' 'remov' 'click' 'basenumb' 'guarante' 'visit' 'bodi' 'will'\n",
      " 'numberb' 'price' 'dollar' 'nbsp' 'below' 'lo' 'most' 'send' 'dollarnumb'\n",
      " 'credit' 'wi' 'hour']\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the top 20 largest coefficients\n",
    "top_indices = clf.coef_[0].argsort()[-20:][::-1]\n",
    "\n",
    "# Get the corresponding words from the vocabulary\n",
    "top_spam_contributors = vocab[top_indices]\n",
    "\n",
    "print (top_spam_contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036b45a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len= 65\n",
      "Remaining len= 61\n",
      "44.0 word(s) from vocab are in the tokens.\n",
      "emailSample1.txt is Not Spam\n",
      "\n",
      "Original len= 228\n",
      "Remaining len= 222\n",
      "122.0 word(s) from vocab are in the tokens.\n",
      "emailSample2.txt is Not Spam\n",
      "\n",
      "Original len= 99\n",
      "Remaining len= 97\n",
      "46.0 word(s) from vocab are in the tokens.\n",
      "spamSample1.txt is Spam\n",
      "\n",
      "Original len= 35\n",
      "Remaining len= 31\n",
      "18.0 word(s) from vocab are in the tokens.\n",
      "spamSample2.txt is Spam\n",
      "\n",
      "Latter sample:\n",
      "==================================================\n",
      "Best Buy Viagra Generic Online\n",
      "\n",
      "Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100% Quality & Satisfaction guaranteed!\n",
      "\n",
      "We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
      "http://medphysitcstech.ru\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for sfn in ['emailSample1.txt', 'emailSample2.txt', 'spamSample1.txt', 'spamSample2.txt']:\n",
    "    fn = os.path.join(path, sfn)\n",
    "    content = get_sample(fn)\n",
    "\n",
    "    # Preprocess the sample\n",
    "    preprocessed_sample = preprocess(content, vocab)\n",
    "\n",
    "    # Get prediction (0 for not spam, 1 for spam) using the trained SVM model (clf)\n",
    "    prediction = clf.predict([preprocessed_sample])[0]\n",
    "\n",
    "    print('{} is {}\\n'.format(sfn, ('Not Spam', 'Spam')[prediction]))\n",
    "\n",
    "print('Latter sample:\\n{1}\\n{0}\\n{1}'.format(content, '='*50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075255e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
